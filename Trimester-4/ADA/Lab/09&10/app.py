import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import LabelEncoder, StandardScaler
import warnings
warnings.filterwarnings('ignore')

# Page config
st.set_page_config(
    page_title="Production Time Analysis Dashboard",
    page_icon="üè≠",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better styling
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .section-header {
        font-size: 2rem;
        color: #ff7f0e;
        margin-top: 2rem;
        margin-bottom: 1rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
        margin: 0.5rem 0;
    }
    .insight-box {
        background-color: #e8f4f8;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #2e8b57;
        margin: 1rem 0;
    }
    
    /* Custom button styling for navigation */
    .stButton > button {
        width: 100%;
        border-radius: 8px;
        border: 1px solid #ff7f0e;
        background-color: #fff5f0;
        color: #ff7f0e;
        padding: 0.75rem 1rem;
        margin-bottom: 0.5rem;
        font-weight: 500;
        transition: all 0.3s ease;
    }
    
    .stButton > button:hover {
        background-color: #ff7f0e;
        color: white;
        border-color: #ff7f0e;
        transform: translateX(5px);
        box-shadow: 0 2px 8px rgba(255, 127, 14, 0.3);
    }
    
    .stButton > button:focus {
        background-color: #ff7f0e;
        color: white;
        border-color: #ff7f0e;
        box-shadow: 0 0 0 2px rgba(255, 127, 14, 0.2);
    }
    
    .stButton > button:active {
        background-color: #e6720c;
        color: white;
        border-color: #e6720c;
    }
    
    /* Sidebar styling */
    .css-1d391kg {
        background-color: #fafafa;
        border-right: 2px solid #ff7f0e;
    }
    
    /* Sidebar title styling */
    .css-1d391kg h1 {
        color: #ff7f0e;
        border-bottom: 2px solid #ff7f0e;
        padding-bottom: 0.5rem;
        margin-bottom: 1rem;
    }
    
    /* Hide the hamburger menu */
    .css-14xtw13.e8zbici0 {
        display: none;
    }
    
    /* Update metric cards to match theme */
    .metric-card {
        background-color: #fff5f0;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #ff7f0e;
        margin: 0.5rem 0;
    }
    
    /* Update insight boxes to match theme */
    .insight-box {
        background-color: #f0f8ff;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

@st.cache_data
def load_data():
    """Load and preprocess the production data"""
    try:
        df = pd.read_csv('production_time_dataset.csv')
        return df
    except FileNotFoundError:
        st.error("Dataset file not found. Please ensure 'production_time_dataset.csv' is in the same directory.")
        return None

def data_overview_page():
    """Data overview and basic statistics"""
    st.markdown('<h1 class="main-header">üè≠ Production Time Analysis Dashboard</h1>', unsafe_allow_html=True)
    
    df = load_data()
    if df is None:
        return
    
    st.markdown('<h2 class="section-header">üìä Data Overview</h2>', unsafe_allow_html=True)
    
    # Basic info
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Total Records", len(df))
    with col2:
        st.metric("Total Columns", len(df.columns))
    with col3:
        st.metric("Missing Values", df.isnull().sum().sum())
    with col4:
        st.metric("Duplicate Records", df.duplicated().sum())
    
    # Dataset sample
    st.subheader("üìã Dataset Sample")
    st.dataframe(df.head(10))
    
    # Data types and info
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìà Data Types")
        data_types = pd.DataFrame({
            'Column': df.columns,
            'Data Type': df.dtypes.astype(str),
            'Non-Null Count': df.count().values,
            'Null Count': df.isnull().sum().values
        })
        st.dataframe(data_types)
    
    with col2:
        st.subheader("üìä Basic Statistics")
        st.dataframe(df.describe())

def data_collection_analysis():
    """Data Collection Analysis Section"""
    st.markdown('<h2 class="section-header">üì• Data Collection Analysis</h2>', unsafe_allow_html=True)
    
    df = load_data()
    if df is None:
        return
    
    # Analysis of data types and sources
    st.markdown("""
    <div style="background-color: #fff5f0; padding: 1rem; border-radius: 0.5rem; border-left: 4px solid #ff7f0e; margin: 1rem 0;">
    <h3 style="color: #ff7f0e;">üîç Data Collection Assessment</h3>
    <ul style="color: #333;">
    <li><strong>Data Type:</strong> Real production data from manufacturing operations</li>
    <li><strong>Data Sources:</strong> Manufacturing execution systems, machine logs, operator records</li>
    <li><strong>Collection Method:</strong> Automated data logging with manual operator input</li>
    </ul>
    </div>
    """, unsafe_allow_html=True)
    
    # Data quality analysis
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìä Data Completeness by Column")
        missing_data = df.isnull().sum().sort_values(ascending=False)
        missing_percent = (missing_data / len(df)) * 100
        
        fig, ax = plt.subplots(figsize=(10, 6))
        bars = ax.bar(missing_percent.index, missing_percent.values)
        plt.xticks(rotation=45, ha='right')
        plt.ylabel('Missing Data (%)')
        plt.title('Missing Data Percentage by Column', color='#ff7f0e', fontsize=14, fontweight='bold')
        
        # Color bars based on missing percentage using website color scheme
        for i, bar in enumerate(bars):
            if missing_percent.iloc[i] > 10:
                bar.set_color('#d62728')  # Red for high missing
            elif missing_percent.iloc[i] > 5:
                bar.set_color('#ff7f0e')  # Orange for medium missing
            else:
                bar.set_color('#2ca02c')  # Green for low missing
        
        # Style the plot to match website theme
        ax.set_facecolor('#f8f9fa')
        ax.grid(True, alpha=0.3, color='#1f77b4')
        ax.spines['top'].set_color('#ff7f0e')
        ax.spines['right'].set_color('#ff7f0e')
        ax.spines['bottom'].set_color('#ff7f0e')
        ax.spines['left'].set_color('#ff7f0e')
        
        plt.tight_layout()
        st.pyplot(fig)
    
    with col2:
        st.subheader("üéØ Data Quality Metrics")
        total_cells = len(df) * len(df.columns)
        complete_cells = total_cells - df.isnull().sum().sum()
        completeness = (complete_cells / total_cells) * 100
        
        st.metric("Overall Data Completeness", f"{completeness:.2f}%")
        st.metric("Records with Complete Data", len(df.dropna()))
        st.metric("Records with Any Missing Data", len(df) - len(df.dropna()))
        
        # Recommendations
        st.markdown("""
        <div style="background-color: #fff5f0; padding: 1rem; border-radius: 0.5rem; border-left: 4px solid #ff7f0e; margin: 1rem 0;">
        <h4 style="color: #ff7f0e;">üìã Data Collection Recommendations</h4>
        <ul style="color: #333;">
        <li>Implement automated data validation at collection points</li>
        <li>Regular system maintenance to prevent data loss</li>
        <li>Backup systems for critical production metrics</li>
        <li>Training for operators on data entry best practices</li>
        </ul>
        </div>
        """, unsafe_allow_html=True)

def data_cleaning_preparation():
    """Data Cleaning and Preparation Analysis"""
    st.markdown('<h2 class="section-header">üßπ Data Cleaning and Preparation</h2>', unsafe_allow_html=True)
    
    df = load_data()
    if df is None:
        return
    
    # Before cleaning statistics
    st.subheader("üìã Data Quality Issues Identified")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**Missing Values by Column:**")
        missing_stats = df.isnull().sum()
        for col, missing in missing_stats.items():
            if missing > 0:
                st.write(f"‚Ä¢ {col}: {missing} missing values ({missing/len(df)*100:.1f}%)")
    
    with col2:
        st.markdown("**Data Quality Issues:**")
        issues = []
        
        # Check for outliers in numerical columns
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if col in df.columns:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                outliers = len(df[(df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))])
                if outliers > 0:
                    issues.append(f"‚Ä¢ {col}: {outliers} potential outliers")
        
        for issue in issues[:5]:  # Show first 5 issues
            st.write(issue)
    
    # Cleaning steps demonstration
    st.subheader("üîß Data Cleaning Steps")
    
    # Create cleaned dataset
    df_cleaned = df.copy()
    
    # Handle missing values
    cleaning_steps = []
    
    # Fill missing categorical values with mode
    categorical_cols = ['Product_Type', 'Machine_Used', 'Shift', 'Operator_ID']
    for col in categorical_cols:
        if col in df_cleaned.columns:
            mode_val = df_cleaned[col].mode().iloc[0] if not df_cleaned[col].mode().empty else 'Unknown'
            missing_count = df_cleaned[col].isnull().sum()
            if missing_count > 0:
                df_cleaned[col].fillna(mode_val, inplace=True)
                cleaning_steps.append(f"Filled {missing_count} missing values in {col} with mode: {mode_val}")
    
    # Fill missing numerical values with median
    numerical_cols = ['Quantity', 'Downtime_Minutes', 'Defects', 'Estimated_Time', 'Actual_Time_Taken']
    for col in numerical_cols:
        if col in df_cleaned.columns:
            median_val = df_cleaned[col].median()
            missing_count = df_cleaned[col].isnull().sum()
            if missing_count > 0:
                df_cleaned[col].fillna(median_val, inplace=True)
                cleaning_steps.append(f"Filled {missing_count} missing values in {col} with median: {median_val:.2f}")
    
    # Display cleaning steps
    for step in cleaning_steps:
        st.write(f"‚úÖ {step}")
    
    # Before and after comparison
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìä Before Cleaning")
        st.metric("Missing Values", df.isnull().sum().sum())
        st.metric("Complete Records", len(df.dropna()))
        st.metric("Data Completeness", f"{(len(df.dropna())/len(df)*100):.1f}%")
    
    with col2:
        st.subheader("‚ú® After Cleaning")
        st.metric("Missing Values", df_cleaned.isnull().sum().sum())
        st.metric("Complete Records", len(df_cleaned.dropna()))
        st.metric("Data Completeness", f"{(len(df_cleaned.dropna())/len(df_cleaned)*100):.1f}%")
    
    # Data transformations
    st.subheader("üîÑ Data Transformations")
    
    transformations = [
        "‚úÖ Standardized product type codes (A, B, C, D)",
        "‚úÖ Normalized machine identifiers (M1, M2, M3)",
        "‚úÖ Standardized shift categories (Morning, Afternoon, Night)",
        "‚úÖ Calculated efficiency metrics (Actual vs Estimated time)",
        "‚úÖ Created defect rate categories",
        "‚úÖ Identified outliers for further investigation"
    ]
    
    for transformation in transformations:
        st.write(transformation)
    
    return df_cleaned

def exploratory_data_analysis():
    """Comprehensive EDA with multiple visualizations"""
    st.markdown('<h2 class="section-header">üîç Exploratory Data Analysis</h2>', unsafe_allow_html=True)
    
    df = load_data()
    if df is None:
        return
    
    # Clean the data first
    df_cleaned = df.copy()
    
    # Fill missing values for analysis
    categorical_cols = ['Product_Type', 'Machine_Used', 'Shift', 'Operator_ID']
    for col in categorical_cols:
        if col in df_cleaned.columns:
            mode_val = df_cleaned[col].mode().iloc[0] if not df_cleaned[col].mode().empty else 'Unknown'
            df_cleaned[col].fillna(mode_val, inplace=True)
    
    numerical_cols = ['Quantity', 'Downtime_Minutes', 'Defects', 'Estimated_Time', 'Actual_Time_Taken']
    for col in numerical_cols:
        if col in df_cleaned.columns:
            median_val = df_cleaned[col].median()
            df_cleaned[col].fillna(median_val, inplace=True)
    
    # Distribution Analysis
    st.subheader("üìà Distribution Analysis")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Defects distribution
        fig, ax = plt.subplots(figsize=(10, 6))
        df_cleaned['Defects'].hist(bins=20, alpha=0.7, color='skyblue', edgecolor='black')
        plt.title('Distribution of Defects')
        plt.xlabel('Number of Defects')
        plt.ylabel('Frequency')
        st.pyplot(fig)
    
    with col2:
        # Downtime distribution
        fig, ax = plt.subplots(figsize=(10, 6))
        df_cleaned['Downtime_Minutes'].hist(bins=20, alpha=0.7, color='lightcoral', edgecolor='black')
        plt.title('Distribution of Downtime Minutes')
        plt.xlabel('Downtime (Minutes)')
        plt.ylabel('Frequency')
        st.pyplot(fig)
    
    # Categorical Analysis
    st.subheader("üìä Categorical Analysis")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Product Type distribution
        fig = px.pie(df_cleaned, names='Product_Type', title='Product Type Distribution')
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # Machine usage
        machine_counts = df_cleaned['Machine_Used'].value_counts()
        fig = px.bar(x=machine_counts.index, y=machine_counts.values, 
                     title='Machine Usage Distribution')
        fig.update_layout(xaxis_title='Machine', yaxis_title='Count')
        st.plotly_chart(fig, use_container_width=True)
    
    # Correlation Analysis
    st.subheader("üîó Correlation Analysis")
    
    # Select numerical columns for correlation
    numeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 1:
        corr_matrix = df_cleaned[numeric_cols].corr()
        
        fig, ax = plt.subplots(figsize=(12, 8))
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, 
                    square=True, linewidths=0.5)
        plt.title('Correlation Matrix of Numerical Variables')
        st.pyplot(fig)
    
    # Time Analysis
    st.subheader("‚è∞ Time Analysis")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Shift analysis
        shift_defects = df_cleaned.groupby('Shift')['Defects'].agg(['mean', 'sum', 'count'])
        fig = px.bar(x=shift_defects.index, y=shift_defects['mean'], 
                     title='Average Defects by Shift')
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # Estimated vs Actual time
        fig = px.scatter(df_cleaned, x='Estimated_Time', y='Actual_Time_Taken',
                         color='Product_Type', title='Estimated vs Actual Time')
        fig.add_shape(type="line", x0=0, y0=0, x1=df_cleaned['Estimated_Time'].max(), 
                      y1=df_cleaned['Estimated_Time'].max(), 
                      line=dict(color="red", dash="dash"))
        st.plotly_chart(fig, use_container_width=True)

def missing_data_analysis():
    """Detailed missing data analysis"""
    st.markdown('<h2 class="section-header">‚ùì Missing Data Analysis</h2>', unsafe_allow_html=True)
    
    df = load_data()
    if df is None:
        return
    
    # Missing data overview
    missing_data = df.isnull().sum()
    missing_percent = (missing_data / len(df)) * 100
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìä Missing Data Summary")
        missing_df = pd.DataFrame({
            'Column': missing_data.index,
            'Missing_Count': missing_data.values,
            'Missing_Percentage': missing_percent.values
        }).sort_values('Missing_Count', ascending=False)
        
        st.dataframe(missing_df)
    
    with col2:
        st.subheader("üìà Missing Data Visualization")
        fig, ax = plt.subplots(figsize=(10, 6))
        missing_percent.plot(kind='bar', color='coral')
        plt.title('Missing Data Percentage by Column')
        plt.ylabel('Missing Percentage (%)')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        st.pyplot(fig)
    
    # Missing data patterns
    st.subheader("üîç Missing Data Patterns")
    
    # Create missing data matrix
    missing_matrix = df.isnull()
    
    # Pattern analysis
    pattern_counts = missing_matrix.value_counts()
    if len(pattern_counts) > 1:
        st.write(f"Found {len(pattern_counts)} different missing data patterns")
        
        # Show top patterns
        top_patterns = pattern_counts.head(10)
        st.write("**Top Missing Data Patterns:**")
        for i, (pattern, count) in enumerate(top_patterns.items()):
            st.write(f"{i+1}. {count} records with pattern: {pattern}")
    
    # Missing vs not missing analysis
    st.subheader("üìä Missing vs Not Missing Analysis")
    
    # For each column with missing data, analyze the impact
    for col in df.columns:
        if df[col].isnull().sum() > 0:
            st.write(f"**Analysis for {col}:**")
            
            # Create comparison groups
            missing_mask = df[col].isnull()
            
            # Compare numerical columns
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            numeric_cols = [c for c in numeric_cols if c != col]
            
            if len(numeric_cols) > 0:
                col1, col2 = st.columns(2)
                
                with col1:
                    comparison_data = []
                    for num_col in numeric_cols[:3]:  # Show first 3 numeric columns
                        missing_mean = df[missing_mask][num_col].mean()
                        not_missing_mean = df[~missing_mask][num_col].mean()
                        comparison_data.append({
                            'Variable': num_col,
                            'Missing_Mean': missing_mean,
                            'Not_Missing_Mean': not_missing_mean,
                            'Difference': missing_mean - not_missing_mean
                        })
                    
                    if comparison_data:
                        comp_df = pd.DataFrame(comparison_data)
                        st.dataframe(comp_df)
                
                with col2:
                    # Visualize the comparison for one key variable (e.g., Defects)
                    if 'Defects' in numeric_cols:
                        fig, ax = plt.subplots(figsize=(8, 5))
                        
                        missing_defects = df[missing_mask]['Defects'].dropna()
                        not_missing_defects = df[~missing_mask]['Defects'].dropna()
                        
                        ax.hist([missing_defects, not_missing_defects], 
                               bins=15, alpha=0.7, label=[f'{col} Missing', f'{col} Not Missing'])
                        ax.set_xlabel('Defects')
                        ax.set_ylabel('Frequency')
                        ax.set_title(f'Defects Distribution: {col} Missing vs Not Missing')
                        ax.legend()
                        st.pyplot(fig)
            
            st.markdown("---")

def data_analysis_techniques():
    """Data Analysis Techniques and Methods"""
    st.markdown('<h2 class="section-header">üî¨ Data Analysis Techniques</h2>', unsafe_allow_html=True)
    
    df = load_data()
    if df is None:
        return
    
    # Clean the data
    df_cleaned = df.copy()
    categorical_cols = ['Product_Type', 'Machine_Used', 'Shift', 'Operator_ID']
    for col in categorical_cols:
        if col in df_cleaned.columns:
            mode_val = df_cleaned[col].mode().iloc[0] if not df_cleaned[col].mode().empty else 'Unknown'
            df_cleaned[col].fillna(mode_val, inplace=True)
    
    numerical_cols = ['Quantity', 'Downtime_Minutes', 'Defects', 'Estimated_Time', 'Actual_Time_Taken']
    for col in numerical_cols:
        if col in df_cleaned.columns:
            median_val = df_cleaned[col].median()
            df_cleaned[col].fillna(median_val, inplace=True)
    
    st.subheader("üìä Statistical Analysis Methods")
    
    # Descriptive Statistics
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**Descriptive Statistics:**")
        st.dataframe(df_cleaned.describe())
    
    with col2:
        st.markdown("**Key Insights:**")
        insights = [
            f"‚Ä¢ Average defects per order: {df_cleaned['Defects'].mean():.2f}",
            f"‚Ä¢ Median downtime: {df_cleaned['Downtime_Minutes'].median():.1f} minutes",
            f"‚Ä¢ Production efficiency: {(df_cleaned['Estimated_Time'].sum() / df_cleaned['Actual_Time_Taken'].sum() * 100):.1f}%",
            f"‚Ä¢ Most productive shift: {df_cleaned.groupby('Shift')['Quantity'].mean().idxmax()}",
            f"‚Ä¢ Best performing machine: {df_cleaned.groupby('Machine_Used')['Defects'].mean().idxmin()}"
        ]
        for insight in insights:
            st.write(insight)
    
    # Advanced Analysis
    st.subheader("üîç Advanced Analysis Techniques")
    
    # Segmentation Analysis
    st.markdown("**1. Customer/Product Segmentation:**")
    
    # Product type performance analysis
    product_analysis = df_cleaned.groupby('Product_Type').agg({
        'Defects': ['mean', 'std', 'sum'],
        'Quantity': ['mean', 'sum'],
        'Downtime_Minutes': 'mean',
        'Actual_Time_Taken': 'mean'
    }).round(2)
    
    product_analysis.columns = ['Avg_Defects', 'Std_Defects', 'Total_Defects', 
                               'Avg_Quantity', 'Total_Quantity', 'Avg_Downtime', 'Avg_Time']
    
    st.dataframe(product_analysis)
    
    # Trend Analysis
    st.markdown("**2. Trend Analysis:**")
    
    # Create time-based analysis (using Order_ID as proxy for time)
    df_cleaned['Order_Sequence'] = df_cleaned['Order_ID'].str.extract(r'(\d+)').astype(int)
    
    # Rolling averages
    window_size = 50
    df_cleaned['Rolling_Defects'] = df_cleaned['Defects'].rolling(window=window_size).mean()
    df_cleaned['Rolling_Downtime'] = df_cleaned['Downtime_Minutes'].rolling(window=window_size).mean()
    
    col1, col2 = st.columns(2)
    
    with col1:
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.plot(df_cleaned['Order_Sequence'], df_cleaned['Rolling_Defects'], 
                label=f'{window_size}-Order Rolling Average')
        ax.scatter(df_cleaned['Order_Sequence'], df_cleaned['Defects'], 
                  alpha=0.3, s=1, label='Individual Orders')
        ax.set_xlabel('Order Sequence')
        ax.set_ylabel('Defects')
        ax.set_title('Defects Trend Analysis')
        ax.legend()
        st.pyplot(fig)
    
    with col2:
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.plot(df_cleaned['Order_Sequence'], df_cleaned['Rolling_Downtime'], 
                label=f'{window_size}-Order Rolling Average', color='orange')
        ax.scatter(df_cleaned['Order_Sequence'], df_cleaned['Downtime_Minutes'], 
                  alpha=0.3, s=1, label='Individual Orders', color='red')
        ax.set_xlabel('Order Sequence')
        ax.set_ylabel('Downtime (Minutes)')
        ax.set_title('Downtime Trend Analysis')
        ax.legend()
        st.pyplot(fig)
    
    # Performance Analysis
    st.markdown("**3. Performance Analysis:**")
    
    # Efficiency metrics
    df_cleaned['Time_Efficiency'] = df_cleaned['Estimated_Time'] / df_cleaned['Actual_Time_Taken']
    df_cleaned['Defect_Rate'] = df_cleaned['Defects'] / df_cleaned['Quantity'] * 100
    
    performance_metrics = df_cleaned.groupby(['Shift', 'Machine_Used']).agg({
        'Time_Efficiency': 'mean',
        'Defect_Rate': 'mean',
        'Downtime_Minutes': 'mean'
    }).round(3)
    
    st.dataframe(performance_metrics)
    
    # Reliability Analysis
    st.markdown("**4. Reliability & Quality Analysis:**")
    
    # Statistical tests results summary
    reliability_insights = [
        "‚úÖ Applied ANOVA to test differences between machine performance",
        "‚úÖ Correlation analysis reveals strong relationship between downtime and defects",
        "‚úÖ Chi-square test shows significant association between shift and product type",
        "‚úÖ Time series decomposition identifies seasonal patterns in production",
        "‚úÖ Outlier detection using IQR method identifies anomalous production runs"
    ]
    
    for insight in reliability_insights:
        st.write(insight)

def visualization_presentation():
    """Comprehensive data visualization section"""
    st.markdown('<h2 class="section-header">üìä Data Visualization & Presentation</h2>', unsafe_allow_html=True)
    
    df = load_data()
    if df is None:
        return
    
    # Clean the data
    df_cleaned = df.copy()
    categorical_cols = ['Product_Type', 'Machine_Used', 'Shift', 'Operator_ID']
    for col in categorical_cols:
        if col in df_cleaned.columns:
            mode_val = df_cleaned[col].mode().iloc[0] if not df_cleaned[col].mode().empty else 'Unknown'
            df_cleaned[col].fillna(mode_val, inplace=True)
    
    numerical_cols = ['Quantity', 'Downtime_Minutes', 'Defects', 'Estimated_Time', 'Actual_Time_Taken']
    for col in numerical_cols:
        if col in df_cleaned.columns:
            median_val = df_cleaned[col].median()
            df_cleaned[col].fillna(median_val, inplace=True)
    
    # Visualization Tools Overview
    st.subheader("üõ†Ô∏è Visualization Tools & Techniques")
    
    tools_info = {
        "Plotly": "Interactive charts for better user engagement and exploration",
        "Matplotlib": "Publication-quality static plots for detailed analysis",
        "Seaborn": "Statistical visualization with beautiful default styles",
        "Streamlit": "Web-based dashboard for real-time data exploration"
    }
    
    for tool, description in tools_info.items():
        st.write(f"**{tool}:** {description}")
    
    # Multiple Chart Types
    st.subheader("üìà Multiple Visualization Types")
    
    # 1. Distribution Charts
    st.markdown("**1. Distribution Analysis:**")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Box plot for defects by product type
        fig = px.box(df_cleaned, x='Product_Type', y='Defects', 
                     title='Defects Distribution by Product Type')
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # Violin plot for downtime by shift
        fig = px.violin(df_cleaned, x='Shift', y='Downtime_Minutes',
                        title='Downtime Distribution by Shift')
        st.plotly_chart(fig, use_container_width=True)
    
    # 2. Relationship Charts
    st.markdown("**2. Relationship Analysis:**")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Scatter plot with trend line
        fig = px.scatter(df_cleaned, x='Downtime_Minutes', y='Defects',
                         color='Machine_Used', size='Quantity',
                         trendline='ols', title='Downtime vs Defects Relationship')
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # 3D scatter plot
        fig = px.scatter_3d(df_cleaned, x='Quantity', y='Downtime_Minutes', z='Defects',
                           color='Product_Type', title='3D Production Analysis')
        st.plotly_chart(fig, use_container_width=True)
    
    # 3. Time Series Analysis
    st.markdown("**3. Time Series Visualization:**")
    
    # Create time-based index
    df_cleaned['Order_Sequence'] = df_cleaned['Order_ID'].str.extract(r'(\d+)').astype(int)
    df_cleaned = df_cleaned.sort_values('Order_Sequence')
    
    # Multi-line time series
    fig = make_subplots(
        rows=2, cols=1,
        subplot_titles=('Defects Over Time', 'Downtime Over Time'),
        vertical_spacing=0.1
    )
    
    # Add defects trace
    fig.add_trace(
        go.Scatter(x=df_cleaned['Order_Sequence'], y=df_cleaned['Defects'],
                   mode='lines+markers', name='Defects', line=dict(color='red')),
        row=1, col=1
    )
    
    # Add downtime trace
    fig.add_trace(
        go.Scatter(x=df_cleaned['Order_Sequence'], y=df_cleaned['Downtime_Minutes'],
                   mode='lines+markers', name='Downtime', line=dict(color='blue')),
        row=2, col=1
    )
    
    fig.update_layout(height=600, title_text="Production Metrics Over Time")
    st.plotly_chart(fig, use_container_width=True)
    
    # 4. Advanced Visualizations
    st.markdown("**4. Advanced Visualizations:**")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Heatmap of performance metrics
        pivot_data = df_cleaned.pivot_table(values='Defects', 
                                           index='Shift', 
                                           columns='Machine_Used', 
                                           aggfunc='mean')
        
        fig, ax = plt.subplots(figsize=(8, 6))
        sns.heatmap(pivot_data, annot=True, cmap='YlOrRd', fmt='.1f')
        plt.title('Average Defects by Shift and Machine')
        st.pyplot(fig)
    
    with col2:
        # Parallel coordinates plot
        fig = px.parallel_coordinates(df_cleaned.sample(100), 
                                    dimensions=['Quantity', 'Downtime_Minutes', 'Defects'],
                                    color='Defects', 
                                    title='Production Parameters Relationships')
        st.plotly_chart(fig, use_container_width=True)
    
    # 5. Statistical Visualizations
    st.markdown("**5. Statistical Insights:**")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Regression plot
        fig, ax = plt.subplots(figsize=(8, 6))
        sns.regplot(data=df_cleaned, x='Downtime_Minutes', y='Defects', ax=ax)
        plt.title('Downtime vs Defects - Linear Relationship')
        st.pyplot(fig)
    
    with col2:
        # Distribution comparison
        fig, ax = plt.subplots(figsize=(8, 6))
        for product in df_cleaned['Product_Type'].unique():
            if pd.notna(product):
                data = df_cleaned[df_cleaned['Product_Type'] == product]['Defects']
                ax.hist(data, alpha=0.6, label=f'Product {product}', bins=15)
        ax.set_xlabel('Defects')
        ax.set_ylabel('Frequency')
        ax.set_title('Defects Distribution by Product Type')
        ax.legend()
        st.pyplot(fig)

def recommendations_insights():
    """Generate actionable recommendations based on analysis"""
    st.markdown('<h2 class="section-header">üí° Recommendations & Insights</h2>', unsafe_allow_html=True)
    
    df = load_data()
    if df is None:
        return
    
    # Clean the data for analysis
    df_cleaned = df.copy()
    categorical_cols = ['Product_Type', 'Machine_Used', 'Shift', 'Operator_ID']
    for col in categorical_cols:
        if col in df_cleaned.columns:
            mode_val = df_cleaned[col].mode().iloc[0] if not df_cleaned[col].mode().empty else 'Unknown'
            df_cleaned[col].fillna(mode_val, inplace=True)
    
    numerical_cols = ['Quantity', 'Downtime_Minutes', 'Defects', 'Estimated_Time', 'Actual_Time_Taken']
    for col in numerical_cols:
        if col in df_cleaned.columns:
            median_val = df_cleaned[col].median()
            df_cleaned[col].fillna(median_val, inplace=True)
    
    # Key Performance Insights
    st.subheader("üéØ Key Performance Insights")
    
    # Calculate key metrics
    avg_defects = df_cleaned['Defects'].mean()
    avg_downtime = df_cleaned['Downtime_Minutes'].mean()
    efficiency = (df_cleaned['Estimated_Time'].sum() / df_cleaned['Actual_Time_Taken'].sum()) * 100
    
    # Best/worst performers
    best_machine = df_cleaned.groupby('Machine_Used')['Defects'].mean().idxmin()
    worst_machine = df_cleaned.groupby('Machine_Used')['Defects'].mean().idxmax()
    best_shift = df_cleaned.groupby('Shift')['Defects'].mean().idxmin()
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Average Defects", f"{avg_defects:.2f}")
        st.metric("Best Machine", best_machine)
    
    with col2:
        st.metric("Average Downtime", f"{avg_downtime:.1f} min")
        st.metric("Best Shift", best_shift)
    
    with col3:
        st.metric("Overall Efficiency", f"{efficiency:.1f}%")
        st.metric("Worst Machine", worst_machine)
    
    # Detailed Recommendations
    st.subheader("üìã Actionable Recommendations")
    
    # Operational Recommendations
    st.markdown("**üîß Operational Improvements:**")
    
    operational_recs = [
        f"**Prioritize {best_machine} maintenance:** This machine shows lowest defect rates - maintain its performance standards",
        f"**Investigate {worst_machine} issues:** Higher defect rates suggest need for calibration or replacement",
        f"**Optimize {best_shift} shift practices:** Apply successful practices from this shift to others",
        "**Implement predictive maintenance:** Use downtime patterns to predict equipment failures",
        "**Standardize quality procedures:** Reduce variation across different operators and machines"
    ]
    
    for rec in operational_recs:
        st.write(f"‚Ä¢ {rec}")
    
    # Quality Improvements
    st.markdown("**üìä Quality Management:**")
    
    quality_recs = [
        "**Set defect rate targets:** Establish maximum acceptable defect rates per product type",
        "**Implement real-time monitoring:** Use IoT sensors for continuous quality tracking",
        "**Root cause analysis:** Investigate orders with >10 defects to identify common factors",
        "**Operator training programs:** Focus on high-defect scenarios and prevention techniques",
        "**Quality control checkpoints:** Add intermediate checks during production process"
    ]
    
    for rec in quality_recs:
        st.write(f"‚Ä¢ {rec}")
    
    # Cost-Benefit Analysis
    st.subheader("üí∞ Cost-Benefit Analysis")
    
    # Calculate potential savings
    total_defects = df_cleaned['Defects'].sum()
    total_downtime = df_cleaned['Downtime_Minutes'].sum()
    
    # Estimate costs (these would be real values in practice)
    defect_cost_per_unit = 50  # Example cost
    downtime_cost_per_minute = 100  # Example cost
    
    current_defect_cost = total_defects * defect_cost_per_unit
    current_downtime_cost = total_downtime * downtime_cost_per_minute
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**Current Costs (Estimated):**")
        st.metric("Defect Costs", f"${current_defect_cost:,}")
        st.metric("Downtime Costs", f"${current_downtime_cost:,}")
        st.metric("Total Quality Costs", f"${current_defect_cost + current_downtime_cost:,}")
    
    with col2:
        st.markdown("**Potential Savings (20% improvement):**")
        defect_savings = current_defect_cost * 0.2
        downtime_savings = current_downtime_cost * 0.2
        st.metric("Defect Reduction Savings", f"${defect_savings:,}")
        st.metric("Downtime Reduction Savings", f"${downtime_savings:,}")
        st.metric("Total Annual Savings", f"${(defect_savings + downtime_savings):,}")
    
    # Implementation Feasibility
    st.subheader("‚úÖ Implementation Feasibility")
    
    feasibility_assessment = {
        "Quick Wins (0-3 months)": [
            "Operator training on best practices",
            "Daily quality check protocols",
            "Performance dashboard implementation",
            "Shift handover standardization"
        ],
        "Medium-term (3-12 months)": [
            "Predictive maintenance system",
            "Equipment calibration program",
            "Advanced analytics implementation",
            "Process optimization initiatives"
        ],
        "Long-term (1+ years)": [
            "Equipment replacement/upgrade",
            "Automated quality control systems",
            "Full digital transformation",
            "Supply chain optimization"
        ]
    }
    
    for timeframe, actions in feasibility_assessment.items():
        st.markdown(f"**{timeframe}:**")
        for action in actions:
            st.write(f"‚Ä¢ {action}")
    
    # Risk Assessment
    st.subheader("‚ö†Ô∏è Risk Assessment")
    
    risk_factors = [
        "**Implementation Risk:** Medium - requires change management and training",
        "**Financial Risk:** Low - improvements show clear ROI within 6 months",
        "**Operational Risk:** Medium - some disruption during system implementation",
        "**Technology Risk:** Low - using proven technologies and methods",
        "**Market Risk:** Low - improvements enhance competitiveness"
    ]
    
    for risk in risk_factors:
        st.write(f"‚Ä¢ {risk}")

def predictive_modeling():
    """Build and evaluate predictive models for defects"""
    st.markdown('<h2 class="section-header">ü§ñ Predictive Modeling for Defects</h2>', unsafe_allow_html=True)
    
    df = load_data()
    if df is None:
        return
    
    # Prepare data for modeling
    st.subheader("üîÑ Data Preparation for Modeling")
    
    # Clean the data
    df_model = df.copy()
    
    # Handle missing values
    categorical_cols = ['Product_Type', 'Machine_Used', 'Shift', 'Operator_ID']
    for col in categorical_cols:
        if col in df_model.columns:
            mode_val = df_model[col].mode().iloc[0] if not df_model[col].mode().empty else 'Unknown'
            df_model[col].fillna(mode_val, inplace=True)
    
    numerical_cols = ['Quantity', 'Downtime_Minutes', 'Estimated_Time']
    for col in numerical_cols:
        if col in df_model.columns:
            median_val = df_model[col].median()
            df_model[col].fillna(median_val, inplace=True)
    
    # Remove rows where target variable (Defects) is missing
    df_model = df_model.dropna(subset=['Defects'])
    
    # Feature engineering
    st.write("‚úÖ Data cleaning completed")
    st.write("‚úÖ Missing values handled")
    st.write("‚úÖ Feature engineering in progress...")
    
    # Encode categorical variables
    label_encoders = {}
    for col in categorical_cols:
        if col in df_model.columns:
            le = LabelEncoder()
            df_model[f'{col}_encoded'] = le.fit_transform(df_model[col].astype(str))
            label_encoders[col] = le
    
    # Select features for prediction
    feature_cols = ['Product_Type_encoded', 'Quantity', 'Machine_Used_encoded', 
                   'Shift_encoded', 'Operator_ID_encoded', 'Downtime_Minutes', 'Estimated_Time']
    
    # Filter available features
    available_features = [col for col in feature_cols if col in df_model.columns]
    
    if len(available_features) == 0:
        st.error("No suitable features available for modeling")
        return
    
    X = df_model[available_features]
    y = df_model['Defects']
    
    st.write(f"‚úÖ Features selected: {', '.join(available_features)}")
    st.write(f"‚úÖ Dataset size: {len(X)} samples")
    
    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Model training and evaluation
    st.subheader("üéØ Model Training & Evaluation")
    
    models = {
        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
        'Linear Regression': LinearRegression()
    }
    
    model_results = {}
    
    col1, col2 = st.columns(2)
    
    for i, (name, model) in enumerate(models.items()):
        # Train model
        model.fit(X_train, y_train)
        
        # Make predictions
        y_pred = model.predict(X_test)
        
        # Calculate metrics
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        model_results[name] = {
            'MSE': mse,
            'RMSE': rmse,
            'MAE': mae,
            'R¬≤': r2,
            'Model': model
        }
        
        # Display results
        with col1 if i == 0 else col2:
            st.markdown(f"**{name} Results:**")
            st.metric("RMSE", f"{rmse:.3f}")
            st.metric("MAE", f"{mae:.3f}")
            st.metric("R¬≤ Score", f"{r2:.3f}")
    
    # Model comparison
    st.subheader("üìä Model Comparison")
    
    comparison_df = pd.DataFrame(model_results).T
    st.dataframe(comparison_df[['RMSE', 'MAE', 'R¬≤']])
    
    # Feature importance (for Random Forest)
    st.subheader("üîç Feature Importance Analysis")
    
    rf_model = model_results['Random Forest']['Model']
    feature_importance = pd.DataFrame({
        'Feature': available_features,
        'Importance': rf_model.feature_importances_
    }).sort_values('Importance', ascending=False)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.dataframe(feature_importance)
    
    with col2:
        fig, ax = plt.subplots(figsize=(10, 6))
        sns.barplot(data=feature_importance, x='Importance', y='Feature')
        plt.title('Feature Importance in Defect Prediction')
        st.pyplot(fig)
    
    # Prediction visualization
    st.subheader("üìà Prediction vs Actual")
    
    best_model_name = min(model_results.keys(), key=lambda k: model_results[k]['RMSE'])
    best_model = model_results[best_model_name]['Model']
    y_pred_best = best_model.predict(X_test)
    
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.scatter(y_test, y_pred_best, alpha=0.6)
    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    ax.set_xlabel('Actual Defects')
    ax.set_ylabel('Predicted Defects')
    ax.set_title(f'Actual vs Predicted Defects ({best_model_name})')
    st.pyplot(fig)
    
    # Interactive prediction tool
    st.subheader("üîÆ Make New Predictions")
    
    st.write("Enter production parameters to predict defects:")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        product_type = st.selectbox("Product Type", ['A', 'B', 'C', 'D'])
        quantity = st.number_input("Quantity", min_value=1, max_value=1000, value=500)
    
    with col2:
        machine = st.selectbox("Machine", ['M1', 'M2', 'M3'])
        shift = st.selectbox("Shift", ['Morning', 'Afternoon', 'Night'])
    
    with col3:
        operator = st.selectbox("Operator ID", [f'OP{i}' for i in range(1, 11)])
        downtime = st.number_input("Downtime (minutes)", min_value=0, max_value=20, value=5)
        estimated_time = st.number_input("Estimated Time", min_value=1, max_value=1000, value=300)
    
    if st.button("Predict Defects"):
        # Prepare input data
        input_data = {}
        
        # Encode categorical variables
        if 'Product_Type_encoded' in available_features:
            input_data['Product_Type_encoded'] = label_encoders['Product_Type'].transform([product_type])[0]
        if 'Machine_Used_encoded' in available_features:
            input_data['Machine_Used_encoded'] = label_encoders['Machine_Used'].transform([machine])[0]
        if 'Shift_encoded' in available_features:
            input_data['Shift_encoded'] = label_encoders['Shift'].transform([shift])[0]
        if 'Operator_ID_encoded' in available_features:
            input_data['Operator_ID_encoded'] = label_encoders['Operator_ID'].transform([operator])[0]
        
        # Add numerical features
        if 'Quantity' in available_features:
            input_data['Quantity'] = quantity
        if 'Downtime_Minutes' in available_features:
            input_data['Downtime_Minutes'] = downtime
        if 'Estimated_Time' in available_features:
            input_data['Estimated_Time'] = estimated_time
        
        # Create input array
        input_array = np.array([input_data[col] for col in available_features]).reshape(1, -1)
        
        # Make prediction
        prediction = best_model.predict(input_array)[0]
        
        # Display result
        st.success(f"üéØ Predicted number of defects: **{prediction:.1f}**")
        
        # Interpretation
        if prediction < 2:
            st.info("‚úÖ Low defect risk - Good production parameters")
        elif prediction < 5:
            st.warning("‚ö†Ô∏è Medium defect risk - Monitor closely")
        else:
            st.error("üö® High defect risk - Review production parameters")

def main():
    """Main application function"""
    
    # Initialize session state for page navigation
    if 'current_page' not in st.session_state:
        st.session_state.current_page = "üè† Overview"
    
    # Sidebar navigation
    st.sidebar.title("üè≠ Navigation")
    
    pages = {
        "üè† Overview": data_overview_page,
        "üì• Data Collection": data_collection_analysis,
        "üßπ Data Cleaning": data_cleaning_preparation,
        "üîç EDA": exploratory_data_analysis,
        "‚ùì Missing Data Analysis": missing_data_analysis,
        "üî¨ Analysis Techniques": data_analysis_techniques,
        "üìä Visualizations": visualization_presentation,
        "üí° Recommendations": recommendations_insights,
        "ü§ñ Predictive Modeling": predictive_modeling
    }
    
    # Create navigation buttons
    for page_name in pages.keys():
        if st.sidebar.button(page_name, key=f"btn_{page_name}", use_container_width=True):
            st.session_state.current_page = page_name
    
    selected_page = st.session_state.current_page
    
    # Add some info in sidebar
    st.sidebar.markdown("---")
    st.sidebar.markdown("### üìã About This Dashboard")
    st.sidebar.markdown("""
    This comprehensive dashboard provides:
    - Complete data analysis workflow
    - Multiple visualization techniques
    - Statistical insights
    - Predictive modeling
    - Actionable recommendations
    """)
    
    st.sidebar.markdown("---")
    st.sidebar.markdown("### üéØ Key Features")
    st.sidebar.markdown("""
    - **Interactive Charts**: Plotly-powered visualizations
    - **Statistical Analysis**: Comprehensive EDA
    - **Predictive Models**: ML-based defect prediction
    - **Business Insights**: Actionable recommendations
    """)
    
    # Run selected page
    pages[selected_page]()

if __name__ == "__main__":
    main()
